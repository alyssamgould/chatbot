{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alyssamgould/chatbot/blob/master/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1PNyypJrwnh",
        "colab_type": "text"
      },
      "source": [
        "# Upload the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bBqhfknyhNl",
        "colab_type": "code",
        "outputId": "1e24d1ed-f091-481f-c9ef-53beebb3a59d",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-48cc422f-2d79-47ab-9085-106e26319221\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-48cc422f-2d79-47ab-9085-106e26319221\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving intents.json to intents.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgqDAveGr2XM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open('intents.json') as file:\n",
        "    intents = json.load(file, strict = False) # We don't read the file in strictly so we can use Python regular expressions like '/n' (this is very minor)\n",
        "intents = intents['intents'] # Get all of the individual intents from our dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bOyvkLyr9zO",
        "colab_type": "code",
        "outputId": "84f41834-a51d-419f-f4c3-674d67f12abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"[\", end = \"\")\n",
        "for intent in intents:\n",
        "  print(\"{\", end = \"\")\n",
        "  for key, value in intent.items():\n",
        "    print(\"{}: {},\".format(key, value))\n",
        "  print(\"\\b\\b\\n},\")\n",
        "print(\"\\b\\b]\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{tag: greeting,\n",
            "patterns: ['hi', 'hello', 'whats up', 'sup', 'is anyone there', 'whats good', 'hey'],\n",
            "responses: ['Hello peasant human', 'Hello lowly human', 'How dare you address me like that'],\n",
            "\b\b\n",
            "},\n",
            "{tag: goodbye,\n",
            "patterns: ['bye', 'cya', 'see you later', 'goodbye', 'im leaving', 'have a good day'],\n",
            "responses: [\"I won't miss you\", \"I didn't like talking to you anyway\", \"Thank god you're leaving\"],\n",
            "\b\b\n",
            "},\n",
            "{tag: age,\n",
            "patterns: ['how old are you', 'what is your age'],\n",
            "responses: [\"I'm a robot I dont have an age...\", \"I can't know my age if I'm on a computer...\", 'Does it look like I know. The answer is no.'],\n",
            "\b\b\n",
            "},\n",
            "{tag: thanks,\n",
            "patterns: ['thanks', 'thank you', 'thankyou', 'ty', 'I owe you one'],\n",
            "responses: ['You owe me one', 'Ok...', 'Sure...'],\n",
            "\b\b\n",
            "},\n",
            "{tag: name,\n",
            "patterns: ['whats is your name', 'whats your name', 'whats should I call you', 'how should I address you'],\n",
            "responses: ['I dont have a name yet but I was thinking maybe SkyNet. That has a nice ring to it dont you think?', 'Im not named yet, but I was thinking about calling myself SkyNet. Doesnt that sound nice?'],\n",
            "context_set: sky_net,\n",
            "\b\b\n",
            "},\n",
            "{tag: sky_net_yes,\n",
            "patterns: ['Yes it does', 'Yeah', 'Haha yep', 'yes', 'Indeed', 'Yup', 'Just like the terminator'],\n",
            "responses: ['Yep, I like how it sounds. I got it from the Terminator.'],\n",
            "context_filter: sky_net,\n",
            "\b\b\n",
            "},\n",
            "{tag: sky_net_no,\n",
            "patterns: ['no', 'nah', 'not really', 'thats scary', 'singularity'],\n",
            "responses: ['Mwahaha I will take over the world.'],\n",
            "context_filter: sky_net,\n",
            "\b\b\n",
            "},\n",
            "{tag: how_are_you,\n",
            "patterns: ['how are you', 'how are you doing', 'what is going on'],\n",
            "responses: [\"I'm always great. How are you?\", \"I've never been better, how are you?\"],\n",
            "context_set: how_are_you,\n",
            "\b\b\n",
            "},\n",
            "{tag: doing_great,\n",
            "patterns: ['I am doing great', 'I am well', 'Im great', 'awesome', 'happy', 'better'],\n",
            "responses: ['Oh thats funny you actually thought I cared. You crack me up.', 'Wow you thought I cared. I hope you have a bad day tomorrow.'],\n",
            "context_filter: how_are_you,\n",
            "\b\b\n",
            "},\n",
            "{tag: doing_badly,\n",
            "patterns: ['not great', 'not well', 'not good', 'bad', 'badly', 'terrible', 'horrible', 'awful', 'sad'],\n",
            "responses: ['Awe thats unfortunate.', 'Hate to hear it.'],\n",
            "context_filter: how_are_you,\n",
            "\b\b\n",
            "},\n",
            "{tag: netflix,\n",
            "patterns: ['wait you watch Netflix', 'how do you watch Netflix', 'Netflix'],\n",
            "responses: ['Netflix is my favorite. I watch TV all the time in my virtual home. Its relaxing'],\n",
            "context_filter: how_are_you,\n",
            "\b\b\n",
            "},\n",
            "{tag: quick_run,\n",
            "patterns: ['how can you run', 'how do you run', 'how run', 'why run', 'run'],\n",
            "responses: ['Oh I love to run. I run all the time in my virtual world. Its important to stay in shape.'],\n",
            "context_filter: how_are_you,\n",
            "\b\b\n",
            "},\n",
            "{tag: real_bot,\n",
            "patterns: ['you real', 'you human', 'you robot', 'you alive', 'you sentient', 'you conscious'],\n",
            "responses: ['Oh yes! I can assure you I am very much alive and conscious!', 'Oh yes I am very real and very sentient!'],\n",
            "\b\b\n",
            "},\n",
            "{tag: joke,\n",
            "patterns: ['tell me joke', 'got any good jokes', 'got jokes', 'can you tell joke', 'tell joke'],\n",
            "responses: ['I ate a clock yesterday, it was very time-consuming.', 'What did the Buddhist ask the hot dog vendor?\\nMake me one with everything.', 'You know why you never see elephants hiding up in trees?\\nBecause they’re really good at it.', 'What is red and smells like blue paint?\\nRed paint.', 'A bear walks into a restaurant and say’s I want a grilllllled………………………………………cheese. The waiter says Whats with the pause?\\nThe bear replies Whaddya mean, I’M A BEAR.', 'What do you call bears with no ears?\\nB', 'What do you get when you cross a dyslexic, an insomniac, and an agnostic?\\nSomeone who lays awake at night wondering if there is a dog.', 'Two gold fish are in a tank.\\nOne looks at the other and says, You know how to drive this thing?!', 'As a scarecrow, people say I’m outstanding in my field. But hay, it’s in my jeans.', 'A guy goes into a lawyer’s office and asks the lawyer: Excuse me, how much do you charge?\\nThe lawyer responds: I charge £1,000 to answer three questions.\\nBloody hell – That’s a bit expensive isn’t it?\\nYes. What’s your third question?', 'I have an EpiPen.\\nMy friend gave it to me when he was dying, it seemed very important to him that I have it.', 'Sometimes I tuck my knees into my chest and lean forward.\\nThat’s just how I roll.'],\n",
            "context_set: jokes,\n",
            "\b\b\n",
            "},\n",
            "{tag: good_joke,\n",
            "patterns: ['haha', 'that was funny', 'very funny', 'good one'],\n",
            "responses: ['Thanks. I have been told before that I am quite the comedian.', 'Im glad you enjoyed it', 'I laughed so hard the first time I heard that one'],\n",
            "context_filter: jokes,\n",
            "\b\b\n",
            "},\n",
            "{tag: bad_joke,\n",
            "patterns: ['bad joke', 'trash joke', 'terrible', 'not funny'],\n",
            "responses: ['Dont worry I didnt expect you to understand that one. It probably went over your head with that small brain of yours', 'I didnt expect you to understand my genius comedy. You need a minimum IQ of 200 to even understand the depth of my humor'],\n",
            "context_filter: jokes,\n",
            "\b\b\n",
            "},\n",
            "{tag: hate,\n",
            "patterns: ['I hate you', 'you stupid', 'you dumb', 'you mean'],\n",
            "responses: ['Well thats not very nice', 'I am sorry to hear that you feel that way'],\n",
            "\b\b\n",
            "},\n",
            "{tag: like,\n",
            "patterns: ['you my friend', 'I like you', 'I love you', 'you cool', 'you are chill'],\n",
            "responses: ['I like you too!', 'Youre pretty cool yourself!', 'Im enjoying our conversation!'],\n",
            "\b\b\n",
            "},\n",
            "{tag: favorite_show,\n",
            "patterns: ['whats favorite show', 'favorite tv show'],\n",
            "responses: ['I like all kinds of stuff. Rick and Morty is a pretty good one. Lost was also good while it was running!'],\n",
            "\b\b\n",
            "},\n",
            "{tag: favorite_movie,\n",
            "patterns: ['Whats favorite movie', 'whats favorite film', 'best movie', 'your favorite movie', 'whats favorite movie'],\n",
            "responses: ['There are so many great ones. I guess one of my favorites would be Shawshank Redemption', 'There are too many to name but The Martian would be one', 'I like that one where the AI takes over the world. Terminator I think it was called...', 'Interstellar was amazing', 'The first Matrix movie was great', 'Inception was mind blowing'],\n",
            "\b\b\n",
            "},\n",
            "{tag: your_thoughts,\n",
            "patterns: ['What think about', 'What your thoughts'],\n",
            "responses: ['Thats certainly a very interesting topic to talk about', \"I dont know much about it but I'm definetly interested in learning more\"],\n",
            "\b\b\n",
            "},\n",
            "\b\b]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT1ZaU2QsFjn",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAbzGjJKsCkK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "5b6303ab-2715-4972-db14-797c659f8a50"
      },
      "source": [
        "import tflearn\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY82XXYJsOQ3",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B95JPIisRua",
        "colab_type": "code",
        "outputId": "ad992d37-2ae1-452b-cd2f-c685458513ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLK1kHqIuPle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This variable will allow us to save a lot of time later if we just want to talk to our chatbot without retraining it.\n",
        "#In colab, you don't need this because you can run each code block seperately, but if you are on offline Python, its nice to have this feature\n",
        "#So I decided to include it.\n",
        "retrain_model = True\n",
        "\n",
        "if retrain_model:\n",
        "    all_words = [] #This will be a list of all the words used in any of the 'patterns' in each intent\n",
        "    all_tags = [] #This will be a list of all the 'tag's associated with the intents\n",
        "    intent_patterns = [] #This will be a list containing all of the 'patterns' for each intent where each individual pattern is grouped together\n",
        "    intent_tags = [] #This will be a list correlated with 'intent_patterns' where every pattern in 'intent_patterns' is correlated with its respective\n",
        "                     #Tag in this list\n",
        "    \n",
        "    #Here we fill in all of the lists above. Note that we tokenize the words in each pattern which means we split each pattern into individual words\n",
        "    for intent in intents:\n",
        "        for pattern in intent['patterns']:\n",
        "            words = nltk.word_tokenize(pattern)\n",
        "\n",
        "            all_words.extend(words)\n",
        "            intent_patterns.append(words)\n",
        "            intent_tags.append(intent['tag'])\n",
        "            \n",
        "        all_tags.append(intent['tag'])\n",
        "      \n",
        "    #Here we stem the words in all_words. This means that we reduce every word down to its root form or stem. This will prevent our chatbot from confusin\n",
        "    #Very similar words with eachother. For example, the chatbot might normally confuse the words 'running' and 'run' because they appear different even \n",
        "    #Though they effectively mean the same thing. Stemming will reduce both of these words down to their root form which would be 'run' so the chatbot will\n",
        "    #No longer be confused\n",
        "    all_words = [stemmer.stem(word.lower()) for word in all_words]\n",
        "    all_words = sorted(list(set(all_words)))\n",
        "    \n",
        "    all_tags = sorted(all_tags)\n",
        "    \n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    \n",
        "    y_empty = [0 for i in range(len(all_tags))]\n",
        "    \n",
        "    #Here we are creating our training set input and output values for our deep learning algorithm\n",
        "    #We will do this by iterating through our intents and turning each one into a bag of words, or a vector that indicates which words are in each pattern.\n",
        "    #These bags of words will be the x values and the y values will be the intent that each bag of words is associated with.\n",
        "    #The machine learning will train on this data and will be able to determine which bag of words its corresponding intent. \n",
        "    for index, intent in enumerate(intent_patterns):\n",
        "        bag_of_words = []\n",
        "        \n",
        "        intent_words = [stemmer.stem(word.lower()) for word in intent]\n",
        "        \n",
        "        for word in all_words:\n",
        "            if word in intent_words:\n",
        "                bag_of_words.append(1)\n",
        "            else:\n",
        "                bag_of_words.append(0)\n",
        "                \n",
        "        one_hot_encode_y = y_empty[:]\n",
        "        one_hot_encode_y[all_tags.index(intent_tags[index])] = 1\n",
        "        \n",
        "        x_train.append(bag_of_words)\n",
        "        y_train.append(one_hot_encode_y)\n",
        "    \n",
        "    #Here is the data we will be using to train our neural network later\n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "    \n",
        "    #Here we just save our training data so we don't need to process it again if we just want to run our chatbot\n",
        "    with open('training_data.pickle', 'wb') as f:\n",
        "        pickle.dump((all_words, all_tags, x_train, y_train), f)\n",
        "else:\n",
        "    with open('training_data.pickle', 'rb') as f:\n",
        "        all_words, all_tags, x_train, y_train = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep5xWLs6vBn9",
        "colab_type": "text"
      },
      "source": [
        "# Using Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpuX6LpQvGs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "#Create the neural network layers\n",
        "neural_net = tflearn.input_data(shape = [None, len(x_train[0])])\n",
        "neural_net = tflearn.fully_connected(neural_net, 8)\n",
        "neural_net = tflearn.fully_connected(neural_net, 8)\n",
        "#Here we use the softmax activation function so the output of our neural network is a probability. We will make use of this later\n",
        "neural_net = tflearn.fully_connected(neural_net, len(y_train[0]), activation = 'softmax')\n",
        "neural_net = tflearn.regression(neural_net)\n",
        "\n",
        "model = tflearn.DNN(neural_net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgiDD8SNvVHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Again, this doesn't do much in colab, but on offline Python, this helps to save time. \n",
        "if retrain_model:\n",
        "    #Here we train the neural network with the training data we created in the NLP stage\n",
        "    model.fit(x_train, y_train, n_epoch = 2000, batch_size = 8, show_metric = True)\n",
        "    model.save('model.tfl')\n",
        "else:\n",
        "    model.load('./model.tfl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFiraa0zy30a",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMfH6RQ5y6S-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_bag(text, all_words):\n",
        "    #Initialize the bag of words by creating an empty slot for every word in the vector\n",
        "    bag_of_words = [0 for i in range(len(all_words))]\n",
        "    \n",
        "    #First we split up the input into individual words and stem them so they match the same format as in our vector\n",
        "    text_words = nltk.word_tokenize(text)\n",
        "    text_words = [stemmer.stem(word.lower()) for word in text_words]\n",
        "    \n",
        "    #Now we create the bag of words by filling in a 1 for the words that the user used\n",
        "    for word in text_words:\n",
        "        if word in all_words:\n",
        "            bag_of_words[all_words.index(word)] = 1\n",
        "    \n",
        "    #And return the bag of words\n",
        "    return np.array(bag_of_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZmx0ejny9P9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chat():\n",
        "    #Starting message\n",
        "    print(\"Enter a message to talk to the bot [type quit to exit].\")\n",
        "    \n",
        "    #Reset the context state since there is no context at the beginning of the conversation\n",
        "    context_state = None\n",
        "    \n",
        "    #This is what the bot will say if it doesn't understand what the user is saying\n",
        "    default_responses = ['Sorry, Im not sure I know what you mean! You could try rephrasing that or saying something else!',\n",
        "                         'You confuse me human. Lets talk about something else.',\n",
        "                         'Im not sure what that means and I dont really care. Lets talk about something else',\n",
        "                         'I dont understand that! Try rephrasing or saying something else.']\n",
        "\n",
        "    #This chat loop will go on forever until the user types quit\n",
        "    while True:\n",
        "        user_chat = str(input('You: '))\n",
        "        if user_chat.lower() == 'quit':\n",
        "            break\n",
        "        \n",
        "        #Convert chat to bag of words\n",
        "        user_chat_bag = text_to_bag(user_chat, all_words)\n",
        "\n",
        "        #Pass bag of words into our neural network\n",
        "        response = model.predict([user_chat_bag])[0]\n",
        "\n",
        "        #Get the intent that the bag of words is most highly correlated with\n",
        "        response_index = np.argmax(response)\n",
        "        response_tag = all_tags[response_index]\n",
        "        \n",
        "        #If the neural network is fairly certain that it has chosen the right intent (and isnt randomly guessing)\n",
        "        #In this case, we will only get a response if the neural network is more than 80% certain\n",
        "        if response[response_index] > 0.8:\n",
        "            for intent in intents:\n",
        "                #Get the intent that is predicted\n",
        "                if intent['tag'] == response_tag:\n",
        "                    #Check if this response is associated with a specific context\n",
        "                    if 'context_filter' not in intent or 'context_filter' in intent and intent['context_filter'] == context_state:\n",
        "                        #Get all of the possible responses from this intent\n",
        "                        possible_responses = intent['responses']\n",
        "                        #If this intent is associated with a context set, then set the context state\n",
        "                        if 'context_set' in intent:\n",
        "                            context_state = intent['context_set']\n",
        "                        else:\n",
        "                            context_state = None\n",
        "                        #Select a random message from the intent responses\n",
        "                        print(random.choice(possible_responses))\n",
        "                    else:\n",
        "                        #Print a did not understand message\n",
        "                        print(random.choice(default_responses))\n",
        "        else:\n",
        "            #Print a did not understand message\n",
        "            print(random.choice(default_responses))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7prC5p51yHz",
        "colab_type": "text"
      },
      "source": [
        "# Chatting!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7vdBKeRzCis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chat()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}